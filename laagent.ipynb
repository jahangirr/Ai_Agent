{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8240568",
   "metadata": {},
   "source": [
    "Aget related to Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37633780",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9ef0b36b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RateLimitError",
     "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRateLimitError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[130]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe weather in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m is sunny.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     11\u001b[39m agent = create_agent(\n\u001b[32m     12\u001b[39m  model=\u001b[33m\"\u001b[39m\u001b[33mgpt-3.5-turbo\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     13\u001b[39m  tools = [get_weather],\n\u001b[32m     14\u001b[39m  system_prompt=\u001b[33m\"\u001b[39m\u001b[33mYou are a helpful assistant.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mWhat is the weather in New York?\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1131\u001b[39m, in \u001b[36mcreate_agent.<locals>.model_node\u001b[39m\u001b[34m(state, runtime)\u001b[39m\n\u001b[32m   1118\u001b[39m request = ModelRequest(\n\u001b[32m   1119\u001b[39m     model=model,\n\u001b[32m   1120\u001b[39m     tools=default_tools,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1126\u001b[39m     runtime=runtime,\n\u001b[32m   1127\u001b[39m )\n\u001b[32m   1129\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wrap_model_call_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1130\u001b[39m     \u001b[38;5;66;03m# No handlers - execute directly\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     response = \u001b[43m_execute_model_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1133\u001b[39m     \u001b[38;5;66;03m# Call composed handler with base handler\u001b[39;00m\n\u001b[32m   1134\u001b[39m     response = wrap_model_call_handler(request, _execute_model_sync)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain\\agents\\factory.py:1102\u001b[39m, in \u001b[36mcreate_agent.<locals>._execute_model_sync\u001b[39m\u001b[34m(request)\u001b[39m\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m request.system_message:\n\u001b[32m   1100\u001b[39m     messages = [request.system_message, *messages]\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m output = \u001b[43mmodel_\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name:\n\u001b[32m   1104\u001b[39m     output.name = name\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5557\u001b[39m, in \u001b[36mRunnableBindingBase.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5550\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5551\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m   5552\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5555\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5556\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5557\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbound\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5558\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   5559\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5560\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m{\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5561\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1380\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1378\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mhttp_response\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1379\u001b[39m         e.response = raw_response.http_response  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1380\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[32m   1381\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1382\u001b[39m     \u001b[38;5;28mself\u001b[39m.include_response_headers\n\u001b[32m   1383\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m raw_response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1384\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(raw_response, \u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1385\u001b[39m ):\n\u001b[32m   1386\u001b[39m     generation_info = {\u001b[33m\"\u001b[39m\u001b[33mheaders\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response.headers)}\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_openai\\chat_models\\base.py:1375\u001b[39m, in \u001b[36mBaseChatOpenAI._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1368\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _construct_lc_result_from_responses_api(\n\u001b[32m   1369\u001b[39m             response,\n\u001b[32m   1370\u001b[39m             schema=original_schema_obj,\n\u001b[32m   1371\u001b[39m             metadata=generation_info,\n\u001b[32m   1372\u001b[39m             output_version=\u001b[38;5;28mself\u001b[39m.output_version,\n\u001b[32m   1373\u001b[39m         )\n\u001b[32m   1374\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1375\u001b[39m         raw_response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwith_raw_response\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1376\u001b[39m         response = raw_response.parse()\n\u001b[32m   1377\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\openai\\_legacy_response.py:364\u001b[39m, in \u001b[36mto_raw_response_wrapper.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    360\u001b[39m extra_headers[RAW_RESPONSE_HEADER] = \u001b[33m\"\u001b[39m\u001b[33mtrue\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    362\u001b[39m kwargs[\u001b[33m\"\u001b[39m\u001b[33mextra_headers\u001b[39m\u001b[33m\"\u001b[39m] = extra_headers\n\u001b[32m--> \u001b[39m\u001b[32m364\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(LegacyAPIResponse[R], \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\openai\\_utils\\_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:1192\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1145\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1146\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1147\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1189\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   1190\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1191\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1197\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1198\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1199\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1200\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_retention\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_retention\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1214\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1215\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1216\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1217\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1218\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1219\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1220\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1221\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1234\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1236\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1237\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1238\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1239\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1044\u001b[39m             err.response.read()\n\u001b[32m   1046\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1047\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1049\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1051\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mRateLimitError\u001b[39m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
      "During task with name 'model' and id 'be5164f3-d877-1516-2176-dee53b094afc'"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "def get_weather(city:str)->str:\n",
    "    \"\"\"Get the weather in a city.\"\"\"\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "agent = create_agent(\n",
    " model=\"gpt-3.5-turbo\",\n",
    " tools = [get_weather],\n",
    " system_prompt=\"You are a helpful assistant.\"\n",
    ")\n",
    "agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in New York?\"}]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b36e6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The capital of Bangladesh is **Dhaka**.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_google_genai import GoogleGenerativeAI , ChatGoogleGenerativeAI\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "response =  llm.invoke(\"what is capital of Bangladesh?\")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b114b666",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"<think>\\nOkay, the user is asking for the capital of Bangladesh. Let me start by recalling what I know. I remember that Bangladesh is a country in South Asia. The capital isn't Dhaka? Wait, no, Dhaka is a major city there. Let me confirm.\\n\\nI think the capital is Dhaka. But wait, sometimes cities have different names or there might be confusion with other cities. Let me think. Bangladesh was part of Pakistan before 1971, and then it became independent. The capital during the Pakistan era was Dhaka, right? After independence, they kept Dhaka as the capital. So yes, Dhaka is the capital. But maybe there's a more official name? Like, sometimes capitals have a different official designation. For example, Paris is the capital of France, but the country is called France. In Bangladesh's case, the capital is Dhaka. I should also check if there's any other city that might be considered, like Chittagong or something else. No, Chittagong is a major port city but not the capital. So the answer should be Dhaka. Let me make sure there's no recent change. I don't recall any recent capital relocations in Bangladesh. So the capital is Dhaka. I should also mention that it's located in the central part of the country and maybe note its significance as a cultural and economic hub. That should cover the user's question.\\n</think>\\n\\nThe capital of Bangladesh is **Dhaka**. \\n\\n**Key Details:**\\n- **Location**: Dhaka is situated in the central-eastern part of Bangladesh, along the banks of the Buriganga River.\\n- **Significance**: It is the largest city in the country and serves as the political, cultural, and economic hub of Bangladesh.\\n- **Population**: As of recent estimates, Dhaka has a population of over 20 million people, making it one of the most populous cities in the world.\\n- **History**: Dhaka became the capital of East Pakistan (now Bangladesh) in 1951 and remained the capital after Bangladesh's independence in 1971.\\n\\nLet me know if you'd like additional details! \""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "llm = init_chat_model(\"groq:qwen/qwen3-32b\")\n",
    "\n",
    "response = llm.invoke(\"what is capital of Bangladesh?\")\n",
    "\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d99d492c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AI, or Artificial Intelligence, refers to the simulation of human intelligence in machines that are programmed to think and learn like humans. The term can also be applied to any machine that exhibits traits associated with a human mind such as learning and problem-solving.\n",
      "\n",
      "AI technology is based on the principle of creating algorithms that can process data, identify patterns, and make decisions or predictions based on that data. This allows AI systems to perform tasks that would typically require human intelligence, such as:\n",
      "\n",
      "1. **Learning**: AI systems can learn from data and improve their performance over time.\n",
      "2. **Problem-solving**: AI systems can analyze problems and find solutions.\n",
      "3. **Reasoning**: AI systems can draw conclusions and make decisions based on data and rules.\n",
      "4. **Perception**: AI systems can interpret and understand data from sensors, such as images, speech, and text.\n",
      "\n",
      "There are several types of AI, including:\n",
      "\n",
      "1. **Narrow or Weak AI**: Designed to perform a specific task, such as facial recognition or language translation.\n",
      "2. **General or Strong AI**: A hypothetical AI system that possesses the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence.\n",
      "3. **Superintelligence**: A hypothetical AI system that significantly surpasses human intelligence in all domains.\n",
      "\n",
      "AI is used in a wide range of applications, including:\n",
      "\n",
      "1. **Virtual assistants**: Siri, Alexa, and Google Assistant.\n",
      "2. **Image recognition**: Self-driving cars and facial recognition software.\n",
      "3. **Natural language processing**: Chatbots and language translation software.\n",
      "4. **Predictive analytics**: Forecasting and decision-making in industries such as finance and healthcare.\n",
      "\n",
      "Overall, AI has the potential to revolutionize many aspects of our lives, from healthcare and education to transportation and entertainment.\n"
     ]
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "response = model.invoke(\"What is AI?\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a52b72dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Bird|s| have| colorful| feathers| for| several| reasons|,| which| can| be| broadly| categorized| into| three| main| areas|:| mating| and| reproduction|,| communication|,| and| camouflage|.\n",
      "\n",
      "|1|.| **|M|ating| and| Rep|roduction|**:| Color|ful| feathers| play| a| crucial| role| in| attracting| a| mate| and| establishing| dominance| within| a| species|.| M|ales| often| display| vibrant| plum|age| to| showcase| their| health|,| genetic| quality|,| and| suitability| as| a| partner|.| This| is| known| as| \"|mate| choice|\"| or| \"|sexual| selection|.\"| For| example|,| pe|ac|ocks| are| famous| for| their| stunning| ir|ides|cent| tails|,| which| they| use| to| attract| females| and| establish| dominance| over| other| males|.\n",
      "\n",
      "|2|.| **|Communication|**:| Color|ful| feathers| serve| as| a| means| of| non|-ver|bal| communication| among| birds|.| They| can| convey| information| about| an| individual|'s| identity|,| sex|,| age|,| social| status|,| and| intentions|.| For| instance|,| some| species| use| specific| color| patterns| to| signal| aggression|,| court|ship|,| or| warning| calls|.| This| visual| language| helps| birds| navigate| complex| social| hier|arch|ies| and| avoid| conflicts|.\n",
      "\n",
      "|3|.| **|Cam|ouflage|**:| While| it| may| seem| counter|int|uitive|,| some| birds| have| colorful| feathers| that| help| them| blend| in| with| their| surroundings|.| This| is| often| referred| to| as| \"|ap|ose|matic| color|ation|.\"| For| example|,| certain| species| of| birds|-of|-par|ad|ise| have| bright| plum|age| that| helps| them| blend| in| with| the| bright| colors| of| their| rain|forest| habitats|.| In| other| cases|,| birds| may| have| color| patterns| that| disrupt| their| outline|,| making| it| harder| for| predators| to| spot| them|.\n",
      "\n",
      "|Additional| factors| contributing| to| colorful| feathers| include|:\n",
      "\n",
      "|-| **|P|ig|mentation|**:| The| production| of| melan|in|,| car|oten|oids|,| and| other| pig|ments| in| feathers| can| result| in| a| wide| range| of| colors|.\n",
      "|-| **|Struct|ural| color|ation|**:| The| microscopic| structure| of| feathers| can| create| ir|ides|cent| colors| through| the| way| light| interacts| with| the| microscopic| scales|.\n",
      "|-| **|Ev|olution|ary| pressures|**:| The| evolution| of| colorful| feathers| can| be| driven| by| various| selective| pressures|,| such| as| pred|ation|,| competition| for| resources|,| and| environmental| factors|.\n",
      "\n",
      "|In| summary|,| birds| have| colorful| feathers| due| to| a| combination| of| factors|,| including| mating| and| reproduction|,| communication|,| camouflage|,| and| the| complex| inter|play| of| pig|mentation|,| structural| color|ation|,| and| evolutionary| pressures|.|||"
     ]
    }
   ],
   "source": [
    "for chunk in model.stream(\"why birds have colourful feathers\"):\n",
    "    print(chunk.text,end=\"|\",flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2c55f6f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of Bangladesh is Dhaka.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 9, 'prompt_tokens': 42, 'total_tokens': 51, 'completion_time': 0.005954666, 'completion_tokens_details': None, 'prompt_time': 0.002483457, 'prompt_tokens_details': None, 'queue_time': 0.055863713, 'total_time': 0.008438123}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b65b3-600d-7f21-bf18-6155e69b3e8c-0' usage_metadata={'input_tokens': 42, 'output_tokens': 9, 'total_tokens': 51}\n",
      "content='The capital of Pakistan is Islamabad.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 41, 'total_tokens': 49, 'completion_time': 0.007042156, 'completion_tokens_details': None, 'prompt_time': 0.001942637, 'prompt_tokens_details': None, 'queue_time': 0.055415673, 'total_time': 0.008984793}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b65b3-6011-73d2-ba79-2b6b69ed3bd0-0' usage_metadata={'input_tokens': 41, 'output_tokens': 8, 'total_tokens': 49}\n",
      "content='The capital of Nepal is Kathmandu.' additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 42, 'total_tokens': 52, 'completion_time': 0.011601338, 'completion_tokens_details': None, 'prompt_time': 0.001924096, 'prompt_tokens_details': None, 'queue_time': 0.055228834, 'total_time': 0.013525434}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b65b3-6013-7843-82a4-5c586920509b-0' usage_metadata={'input_tokens': 42, 'output_tokens': 10, 'total_tokens': 52}\n"
     ]
    }
   ],
   "source": [
    "for bresponse in model.batch([\n",
    "    \"what is capital of bangadesh?\",\n",
    "    \"what is capital of pakistan?\",\n",
    "    \"what is capital of nepal?\"\n",
    "],config={'max_concurrency': 5}):\n",
    "    print(bresponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "acc45dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_name : get_weather\n",
      "tool_input : {'city': 'New York'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def get_weather(city:str) -> int:\n",
    "    \"\"\"get the weather at a location\"\"\"\n",
    "    return f\"The weather in {city} is sunny.\"\n",
    "\n",
    "model_tool = model.bind_tools([get_weather])\n",
    "response = model_tool.invoke(\"What is the weather in New York?\")\n",
    "for tool_call in response.tool_calls:\n",
    "    print(f\"tool_name : {tool_call[\"name\"]}\")\n",
    "    print(f\"tool_input : {tool_call[\"args\"]}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e4f9a956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'What is the weather in New York?'}, AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'q7ehm4r3h', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 220, 'total_tokens': 235, 'completion_time': 0.033793251, 'completion_tokens_details': None, 'prompt_time': 0.02876088, 'prompt_tokens_details': None, 'queue_time': 0.05223927, 'total_time': 0.062554131}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b65f5-9004-7703-85a7-f87a8dca26bf-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'New York'}, 'id': 'q7ehm4r3h', 'type': 'tool_call'}], usage_metadata={'input_tokens': 220, 'output_tokens': 15, 'total_tokens': 235})]\n",
      "[{'role': 'user', 'content': 'What is the weather in New York?'}, AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'q7ehm4r3h', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'get_weather'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 220, 'total_tokens': 235, 'completion_time': 0.033793251, 'completion_tokens_details': None, 'prompt_time': 0.02876088, 'prompt_tokens_details': None, 'queue_time': 0.05223927, 'total_time': 0.062554131}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b65f5-9004-7703-85a7-f87a8dca26bf-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'New York'}, 'id': 'q7ehm4r3h', 'type': 'tool_call'}], usage_metadata={'input_tokens': 220, 'output_tokens': 15, 'total_tokens': 235}), ToolMessage(content='The weather in New York is sunny.', name='get_weather', tool_call_id='q7ehm4r3h')]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "message = [{\"role\": \"user\", \"content\": \"What is the weather in New York?\"}]\n",
    "ai_message = model_tool.invoke(message)\n",
    "message.append(ai_message)\n",
    "print(message)\n",
    "for tool_call in ai_message.tool_calls:\n",
    "    tool_result = get_weather.invoke(tool_call)\n",
    "    message.append(tool_result)\n",
    "    print(message)\n",
    "\n",
    "final_result = model_tool.invoke(message)\n",
    "print(final_result.content)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "053719ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_tokens': 53, 'output_tokens': 14, 'total_tokens': 67}\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage, HumanMessage,SystemMessage\n",
    "ai_message = AIMessage(content=\"I am ready to relp you\")\n",
    "message = [\n",
    "    SystemMessage(content=\"you are a helpful assistant\"),\n",
    "    HumanMessage(content=\"what is 2+2\"),\n",
    "    ai_message\n",
    "]\n",
    "\n",
    "respone = model.invoke(message)\n",
    "print(respone.usage_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44008d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2 + 2 = 4', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 8, 'prompt_tokens': 41, 'total_tokens': 49, 'completion_time': 0.006477372, 'completion_tokens_details': None, 'prompt_time': 0.001926094, 'prompt_tokens_details': None, 'queue_time': 0.050757226, 'total_time': 0.008403466}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_1151d4f23c', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6612-645c-7640-b12a-127469317ba4-0', usage_metadata={'input_tokens': 41, 'output_tokens': 8, 'total_tokens': 49})"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_humanMessage = HumanMessage(content=\"what is 2+2\" , name=\"my_name\",id=\"my_id_123\")\n",
    "model.invoke([my_humanMessage])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d97b3811",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please note that this is a fictional response as I'm a large language model, I don't have real-time access to current weather conditions. However, I can suggest some ways for you to find out the current weather in New York.\n",
      "\n",
      "You can check the weather on websites such as:\n",
      "\n",
      "- AccuWeather (accuweather.com)\n",
      "- Weather.com (weather.com)\n",
      "- National Weather Service (weather.gov)\n",
      "\n",
      "Or you can use a voice assistant like Siri, Google Assistant, or Alexa to get the current weather in New York.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.messages import ToolMessage\n",
    "ai_message = AIMessage(content=\"\",\n",
    "     tool_calls=[{\n",
    "    \"name\":\"get_weather\",\"args\":{\"location\":\"New York\"},\"id\":\"my_id_123\"}])\n",
    "\n",
    "weather_result = \"The weather in New York is sunny with a temperature of 22 degrees Celsius.\"\n",
    "mo_tool = ToolMessage(content=weather_result,tool_call_id=\"my_id_123\")\n",
    "\n",
    "mo_message = [\n",
    "    HumanMessage(content=\"Hello, how is the weather in New York?\"),\n",
    "    ai_message,\n",
    "    mo_tool\n",
    "]\n",
    "\n",
    "result = model.invoke(mo_message)\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "08d54c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please note that the weather information provided is fictional and for demonstration purposes only. For accurate and up-to-date weather information, I recommend checking a reliable weather forecasting service or website, such as AccuWeather or the National Weather Service.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import AIMessage, HumanMessage, ToolMessage\n",
    "\n",
    "# AI message requesting a tool\n",
    "ai_message = AIMessage(\n",
    "    content=\"\",\n",
    "    tool_calls=[{\n",
    "        \"name\": \"get_weather\",\n",
    "        \"args\": {\"location\": \"New York\"},\n",
    "        \"id\": \"my_id_123\"\n",
    "    }]\n",
    ")\n",
    "\n",
    "# Tool response\n",
    "weather_result = \"The weather in New York is sunny with a temperature of 22C.\"\n",
    "tool_message = ToolMessage(\n",
    "    content=weather_result,\n",
    "    tool_call_id=\"my_id_123\"\n",
    ")\n",
    "\n",
    "# Conversation history (must be a LIST)\n",
    "messages = [\n",
    "    HumanMessage(content=\"Hello, how is the weather in New York?\"),\n",
    "    ai_message,\n",
    "    tool_message,\n",
    "]\n",
    "\n",
    "# Invoke model\n",
    "result = model.invoke(messages)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "136ad5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Movie(title='Titanic', director='James Cameron', year=1997, rating=7.5)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title : str = Field(description=\"Title of the movie\")\n",
    "    director : str = Field(description=\"Director of the movie\")\n",
    "    year : int = Field(description=\"Year of the movie\")\n",
    "    rating : float = Field(description=\"Rating of the movie\")\n",
    "\n",
    "model_structure_output = model.with_structured_output(Movie)\n",
    "\n",
    "result = model_structure_output.invoke(\"Tell me about the Titanic movie ?\")\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42c6f874",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'kr3t0c2k9', 'function': {'arguments': '{\"director\":\"James Cameron\",\"rating\":7.5,\"title\":\"Titanic\",\"year\":1997}', 'name': 'Movie'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 33, 'prompt_tokens': 272, 'total_tokens': 305, 'completion_time': 0.043587342, 'completion_tokens_details': None, 'prompt_time': 0.020454679, 'prompt_tokens_details': None, 'queue_time': 0.051324801, 'total_time': 0.064042021}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_f757f4b0bf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b665b-7856-77c3-8416-1496eb13637e-0', tool_calls=[{'name': 'Movie', 'args': {'director': 'James Cameron', 'rating': 7.5, 'title': 'Titanic', 'year': 1997}, 'id': 'kr3t0c2k9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 272, 'output_tokens': 33, 'total_tokens': 305}),\n",
       " 'parsed': Movie(title='Titanic', director='James Cameron', year=1997, rating=7.5),\n",
       " 'parsing_error': None}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "\n",
    "class Movie(BaseModel):\n",
    "    title : str = Field(...,description=\"Title of the movie\")\n",
    "    director : str = Field(description=\"Director of the movie\")\n",
    "    year : int = Field(description=\"Year of the movie\")\n",
    "    rating : float = Field(description=\"Rating of the movie\")\n",
    "\n",
    "model_structure_output = model.with_structured_output(Movie,include_raw=True)\n",
    "\n",
    "result = model_structure_output.invoke(\"Tell me about the Titanic movie ?\")\n",
    "result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "053a1c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': '18n78q16m', 'function': {'arguments': '{\"actors\":[{\"name\":\"Leonardo DiCaprio\",\"role\":\"Jack Dawson\"},{\"name\":\"Kate Winslet\",\"role\":\"Rose DeWitt Bukater\"}],\"budget\":200000000,\"genre\":[\"Romance\",\"Drama\",\"Disaster\"],\"title\":\"Titanic\",\"year\":1997}', 'name': 'Movie'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 81, 'prompt_tokens': 302, 'total_tokens': 383, 'completion_time': 0.094249958, 'completion_tokens_details': None, 'prompt_time': 0.028709245, 'prompt_tokens_details': None, 'queue_time': 0.056069555, 'total_time': 0.122959203}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_ff2b098aaf', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6661-ca5e-7600-82a1-a3f8f9ef5e9f-0', tool_calls=[{'name': 'Movie', 'args': {'actors': [{'name': 'Leonardo DiCaprio', 'role': 'Jack Dawson'}, {'name': 'Kate Winslet', 'role': 'Rose DeWitt Bukater'}], 'budget': 200000000, 'genre': ['Romance', 'Drama', 'Disaster'], 'title': 'Titanic', 'year': 1997}, 'id': '18n78q16m', 'type': 'tool_call'}], usage_metadata={'input_tokens': 302, 'output_tokens': 81, 'total_tokens': 383}),\n",
       " 'parsed': Movie(title='Titanic', year=1997, actors=[Actor(name='Leonardo DiCaprio', role='Jack Dawson'), Actor(name='Kate Winslet', role='Rose DeWitt Bukater')], genre=['Romance', 'Drama', 'Disaster'], budget=200000000.0),\n",
       " 'parsing_error': None}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydantic import BaseModel , Field\n",
    "\n",
    "class Actor(BaseModel):\n",
    "    name: str\n",
    "    role: str\n",
    "class Movie(BaseModel):\n",
    "    title: str\n",
    "    year: int\n",
    "    actors: list[Actor]\n",
    "    genre: list[str]\n",
    "    budget:float|None=Field(None,description=\"Budget of the movie\")\n",
    "\n",
    "\n",
    "model_structure_output = model.with_structured_output(Movie,include_raw=True)\n",
    "\n",
    "result = model_structure_output.invoke(\"Tell me about the Titanic movie ?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "699f2a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'director': 'James Cameron', 'rating': 7.5, 'title': 'Titanic', 'year': 1997}\n"
     ]
    }
   ],
   "source": [
    "from typing_extensions import Annotated , TypedDict\n",
    "\n",
    "class MovieTypeDict(TypedDict):\n",
    "    title : Annotated[str ,..., \"Name of the movie\"]\n",
    "    year : Annotated[int , ... , \"Year of the movie\"]\n",
    "    director : Annotated[str ,..., \"Director of the movie\"]\n",
    "    rating : Annotated[float ,..., \"Rating of the movie\"]\n",
    "\n",
    "structure_response = model.with_structured_output(MovieTypeDict)\n",
    "result = structure_response.invoke( \"Tell me about the movie Titanic?\")\n",
    "print(result)   \n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ffd4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' phone='1234567890' email='jahidb78luvk@gmail.com'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    \"\"\"Contact information for a person.\"\"\"\n",
    "    name: str = Field(description=\"The name of the contact\")\n",
    "    phone: str = Field(description=\"The phone number of the contact\")\n",
    "    email: str = Field(description=\"The email address of the contact\")\n",
    "\n",
    "\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_object=ContactInfo)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Extract contact information from the text below.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Text:\n",
    "{input}\n",
    "\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "res = chain.invoke(\n",
    "    \"Extract contact information for John Doe, jahidb78luvk@gmail.com, 1234567890\"\n",
    ")\n",
    "\n",
    "print(res)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "9c70fa9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' phone='1234567890' email='jahidb78luvk@gmail.com'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.output_parsers import PydanticOutputParser\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.2\n",
    ")\n",
    "\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str = Field(description=\"The name of the contact\")\n",
    "    phone: str = Field(description=\"The phone number of the contact\")\n",
    "    email: str = Field(description=\"The email address of the contact\")\n",
    "\n",
    "\n",
    "model = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "# Base parser\n",
    "base_parser = PydanticOutputParser(pydantic_object=ContactInfo)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "Extract contact information from the text below.\n",
    "\n",
    "Return ONLY valid JSON. No explanation.\n",
    "\n",
    "{format_instructions}\n",
    "\n",
    "Text:\n",
    "{input}\n",
    "\"\"\",\n",
    "    input_variables=[\"input\"],\n",
    "    partial_variables={\n",
    "        \"format_instructions\": base_parser.get_format_instructions()\n",
    "    }\n",
    ")\n",
    "\n",
    "chain = prompt | model | parser\n",
    "\n",
    "res = chain.invoke(\n",
    "    \"Extract contact information for John Doe, jahidb78luvk@gmail.com, 1234567890\"\n",
    ")\n",
    "\n",
    "print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abee943b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.1-8b-instant\",\n",
    "    temperature=0.2  \n",
    ")\n",
    "\n",
    "@tool\n",
    "def get_addition(a:int,b:int)-> str:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return str(a + b)\n",
    "    \n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant that can use tools.\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "\n",
    "llm_bind_tool = llm.bind_tools([get_addition])\n",
    "def custom_tool_call(message):\n",
    "    if not message.tool_calls:\n",
    "        return message.content\n",
    "    tool_call = message.tool_calls[0]\n",
    "    result = get_addition.invoke(tool_call[\"args\"])\n",
    "    return result\n",
    "    \n",
    "\n",
    "chain = (prompt | llm_bind_tool | RunnableLambda(custom_tool_call))\n",
    "\n",
    "chain.invoke({\"input\": \"Add 2 and 4\"})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48010a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages : {'messages': [HumanMessage(content='what is 3 + 4?', additional_kwargs={}, response_metadata={}, id='539974bc-df6e-417f-809b-ce76e4fd4d12'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 3 + 4?\" So I need to add these two numbers. The function provided is add_numbers, which takes two integers, a and b. Let me check the parameters: a and b are both required and should be integers. Here, 3 and 4 are integers, so that\\'s perfect. I should call the add_numbers function with a=3 and b=4. There\\'s no need for any other functions here. Just make sure the JSON is correctly formatted with the right parameters.\\n', 'tool_calls': [{'id': '73cfdfdkf', 'function': {'arguments': '{\"a\":3,\"b\":4}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 141, 'prompt_tokens': 163, 'total_tokens': 304, 'completion_time': 0.225188807, 'completion_tokens_details': {'reasoning_tokens': 111}, 'prompt_time': 0.006538232, 'prompt_tokens_details': None, 'queue_time': 0.158855468, 'total_time': 0.231727039}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8662-7bb2-9082-41e48a488dcb-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 3, 'b': 4}, 'id': '73cfdfdkf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 163, 'output_tokens': 141, 'total_tokens': 304, 'output_token_details': {'reasoning': 111}}), ToolMessage(content='7', name='add_numbers', id='371d03b8-4567-46ba-93ea-a6afdc334c53', tool_call_id='73cfdfdkf'), AIMessage(content='The sum of 3 and 4 is **7**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 3 + 4?\" so I need to add those two numbers. I called the add_numbers function with a=3.0 and b=4.0. The response from the tool was 7. Now I just need to present the answer clearly. Let me check if there\\'s any other step, but since the function already returned the sum, I can directly state the result. So the final answer is 7.\\n'}, response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 207, 'total_tokens': 320, 'completion_time': 0.203939899, 'completion_tokens_details': {'reasoning_tokens': 95}, 'prompt_time': 0.009234957, 'prompt_tokens_details': None, 'queue_time': 0.159549063, 'total_time': 0.213174856}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8894-7072-a4da-c04b84413014-0', usage_metadata={'input_tokens': 207, 'output_tokens': 113, 'total_tokens': 320, 'output_token_details': {'reasoning': 95}})]}\n",
      "messages : 4\n",
      "messages : {'messages': [HumanMessage(content='what is 3 + 4?', additional_kwargs={}, response_metadata={}, id='539974bc-df6e-417f-809b-ce76e4fd4d12'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 3 + 4?\" So I need to add these two numbers. The function provided is add_numbers, which takes two integers, a and b. Let me check the parameters: a and b are both required and should be integers. Here, 3 and 4 are integers, so that\\'s perfect. I should call the add_numbers function with a=3 and b=4. There\\'s no need for any other functions here. Just make sure the JSON is correctly formatted with the right parameters.\\n', 'tool_calls': [{'id': '73cfdfdkf', 'function': {'arguments': '{\"a\":3,\"b\":4}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 141, 'prompt_tokens': 163, 'total_tokens': 304, 'completion_time': 0.225188807, 'completion_tokens_details': {'reasoning_tokens': 111}, 'prompt_time': 0.006538232, 'prompt_tokens_details': None, 'queue_time': 0.158855468, 'total_time': 0.231727039}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8662-7bb2-9082-41e48a488dcb-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 3, 'b': 4}, 'id': '73cfdfdkf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 163, 'output_tokens': 141, 'total_tokens': 304, 'output_token_details': {'reasoning': 111}}), ToolMessage(content='7', name='add_numbers', id='371d03b8-4567-46ba-93ea-a6afdc334c53', tool_call_id='73cfdfdkf'), AIMessage(content='The sum of 3 and 4 is **7**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 3 + 4?\" so I need to add those two numbers. I called the add_numbers function with a=3.0 and b=4.0. The response from the tool was 7. Now I just need to present the answer clearly. Let me check if there\\'s any other step, but since the function already returned the sum, I can directly state the result. So the final answer is 7.\\n'}, response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 207, 'total_tokens': 320, 'completion_time': 0.203939899, 'completion_tokens_details': {'reasoning_tokens': 95}, 'prompt_time': 0.009234957, 'prompt_tokens_details': None, 'queue_time': 0.159549063, 'total_time': 0.213174856}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8894-7072-a4da-c04b84413014-0', usage_metadata={'input_tokens': 207, 'output_tokens': 113, 'total_tokens': 320, 'output_token_details': {'reasoning': 95}}), HumanMessage(content='what is 4 + 5?', additional_kwargs={}, response_metadata={}, id='bc9d4bca-0531-449b-9bb9-dc5e772e2c83'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 4 + 5?\" So I need to use the add_numbers function again. Let me check the parameters. The function requires two integers, a and b. Here, 4 and 5 are the numbers to add. I\\'ll call the function with a=4 and b=5. The previous example used 3 and 4, and the response was 7, so following the same pattern, 4+5 should be 9. I need to make sure the JSON is correctly formatted with the right arguments.\\n', 'tool_calls': [{'id': 'q49rv2wqh', 'function': {'arguments': '{\"a\":4,\"b\":5}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 238, 'total_tokens': 384, 'completion_time': 0.234088123, 'completion_tokens_details': {'reasoning_tokens': 116}, 'prompt_time': 0.009818212, 'prompt_tokens_details': None, 'queue_time': 0.057221758, 'total_time': 0.243906335}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8a7d-7363-84f3-e5d3e5ef9b4c-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 4, 'b': 5}, 'id': 'q49rv2wqh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 238, 'output_tokens': 146, 'total_tokens': 384, 'output_token_details': {'reasoning': 116}}), ToolMessage(content='9', name='add_numbers', id='7280af15-a6e9-40a1-95d4-6ebab6c7d593', tool_call_id='q49rv2wqh'), AIMessage(content='The sum of 4 and 5 is **9**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 4 + 5?\" and I called the add_numbers function with a=4.0 and b=5.0. The response was 9. Now I need to present the answer clearly. Let me check if the function worked correctly. 4 plus 5 is indeed 9, so the function is working as expected. I should state the result in a simple sentence, maybe using bold for the numbers and the result to make it stand out. Let me format the response to be user-friendly.\\n'}, response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 282, 'total_tokens': 411, 'completion_time': 0.231929745, 'completion_tokens_details': {'reasoning_tokens': 111}, 'prompt_time': 0.011722016, 'prompt_tokens_details': None, 'queue_time': 0.158828544, 'total_time': 0.243651761}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8c20-7642-948c-39233c3b2a09-0', usage_metadata={'input_tokens': 282, 'output_tokens': 129, 'total_tokens': 411, 'output_token_details': {'reasoning': 111}})]}\n",
      "messages : 8\n",
      "messages : {'messages': [HumanMessage(content='what is 3 + 4?', additional_kwargs={}, response_metadata={}, id='539974bc-df6e-417f-809b-ce76e4fd4d12'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 3 + 4?\" So I need to add these two numbers. The function provided is add_numbers, which takes two integers, a and b. Let me check the parameters: a and b are both required and should be integers. Here, 3 and 4 are integers, so that\\'s perfect. I should call the add_numbers function with a=3 and b=4. There\\'s no need for any other functions here. Just make sure the JSON is correctly formatted with the right parameters.\\n', 'tool_calls': [{'id': '73cfdfdkf', 'function': {'arguments': '{\"a\":3,\"b\":4}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 141, 'prompt_tokens': 163, 'total_tokens': 304, 'completion_time': 0.225188807, 'completion_tokens_details': {'reasoning_tokens': 111}, 'prompt_time': 0.006538232, 'prompt_tokens_details': None, 'queue_time': 0.158855468, 'total_time': 0.231727039}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8662-7bb2-9082-41e48a488dcb-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 3, 'b': 4}, 'id': '73cfdfdkf', 'type': 'tool_call'}], usage_metadata={'input_tokens': 163, 'output_tokens': 141, 'total_tokens': 304, 'output_token_details': {'reasoning': 111}}), ToolMessage(content='7', name='add_numbers', id='371d03b8-4567-46ba-93ea-a6afdc334c53', tool_call_id='73cfdfdkf'), AIMessage(content='The sum of 3 and 4 is **7**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 3 + 4?\" so I need to add those two numbers. I called the add_numbers function with a=3.0 and b=4.0. The response from the tool was 7. Now I just need to present the answer clearly. Let me check if there\\'s any other step, but since the function already returned the sum, I can directly state the result. So the final answer is 7.\\n'}, response_metadata={'token_usage': {'completion_tokens': 113, 'prompt_tokens': 207, 'total_tokens': 320, 'completion_time': 0.203939899, 'completion_tokens_details': {'reasoning_tokens': 95}, 'prompt_time': 0.009234957, 'prompt_tokens_details': None, 'queue_time': 0.159549063, 'total_time': 0.213174856}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8894-7072-a4da-c04b84413014-0', usage_metadata={'input_tokens': 207, 'output_tokens': 113, 'total_tokens': 320, 'output_token_details': {'reasoning': 95}}), HumanMessage(content='what is 4 + 5?', additional_kwargs={}, response_metadata={}, id='bc9d4bca-0531-449b-9bb9-dc5e772e2c83'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 4 + 5?\" So I need to use the add_numbers function again. Let me check the parameters. The function requires two integers, a and b. Here, 4 and 5 are the numbers to add. I\\'ll call the function with a=4 and b=5. The previous example used 3 and 4, and the response was 7, so following the same pattern, 4+5 should be 9. I need to make sure the JSON is correctly formatted with the right arguments.\\n', 'tool_calls': [{'id': 'q49rv2wqh', 'function': {'arguments': '{\"a\":4,\"b\":5}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 146, 'prompt_tokens': 238, 'total_tokens': 384, 'completion_time': 0.234088123, 'completion_tokens_details': {'reasoning_tokens': 116}, 'prompt_time': 0.009818212, 'prompt_tokens_details': None, 'queue_time': 0.057221758, 'total_time': 0.243906335}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8a7d-7363-84f3-e5d3e5ef9b4c-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 4, 'b': 5}, 'id': 'q49rv2wqh', 'type': 'tool_call'}], usage_metadata={'input_tokens': 238, 'output_tokens': 146, 'total_tokens': 384, 'output_token_details': {'reasoning': 116}}), ToolMessage(content='9', name='add_numbers', id='7280af15-a6e9-40a1-95d4-6ebab6c7d593', tool_call_id='q49rv2wqh'), AIMessage(content='The sum of 4 and 5 is **9**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 4 + 5?\" and I called the add_numbers function with a=4.0 and b=5.0. The response was 9. Now I need to present the answer clearly. Let me check if the function worked correctly. 4 plus 5 is indeed 9, so the function is working as expected. I should state the result in a simple sentence, maybe using bold for the numbers and the result to make it stand out. Let me format the response to be user-friendly.\\n'}, response_metadata={'token_usage': {'completion_tokens': 129, 'prompt_tokens': 282, 'total_tokens': 411, 'completion_time': 0.231929745, 'completion_tokens_details': {'reasoning_tokens': 111}, 'prompt_time': 0.011722016, 'prompt_tokens_details': None, 'queue_time': 0.158828544, 'total_time': 0.243651761}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8c20-7642-948c-39233c3b2a09-0', usage_metadata={'input_tokens': 282, 'output_tokens': 129, 'total_tokens': 411, 'output_token_details': {'reasoning': 111}}), HumanMessage(content='what is 5 + 3?', additional_kwargs={}, response_metadata={}, id='b6974cc9-e5f6-4082-bfcb-d73f68e23f03'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 5 + 3?\" Let me see. They\\'ve previously asked similar questions like 3+4 and 4+5, and each time I used the add_numbers function. The pattern here is straightforward addition.\\n\\nFirst, I need to confirm that the function add_numbers is the right tool here. The function takes two integers, a and b, and returns their sum. The user\\'s question is 5 + 3, which are both integers. So, applying the function makes sense.\\n\\nI should check if there\\'s any reason not to use the function here. But since the user is just asking for the sum, and the function is designed for this exact purpose, it\\'s the correct approach. \\n\\nWait, in the previous interactions, the user input was in the form of \"what is X + Y?\" and I converted those into function calls. This time it\\'s \"5 + 3\", which is the same structure. No complications here. \\n\\nI need to make sure the parameters are correctly formatted. The function requires a and b as integers. 5 and 3 are integers, so that\\'s good. \\n\\nNo edge cases here. The numbers are within a reasonable range, and there\\'s no indication of any errors. Just a simple addition. \\n\\nSo, the next step is to generate the tool call with the arguments a=5 and b=3. Then, once the function returns the result, I can present the answer to the user. \\n\\nI should also remember to format the response correctly, using the JSON structure inside the tool_call tags. Let me double-check the syntax to avoid any formatting errors. \\n\\nYes, the correct JSON would be {\"name\": \"add_numbers\", \"arguments\": {\"a\": 5, \"b\": 3}}. \\n\\nAlright, that\\'s all. The function will handle the rest, and I can provide the answer once the tool responds.\\n', 'tool_calls': [{'id': '73mt5mz4w', 'function': {'arguments': '{\"a\":5,\"b\":3}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 424, 'prompt_tokens': 313, 'total_tokens': 737, 'completion_time': 1.393794658, 'completion_tokens_details': {'reasoning_tokens': 394}, 'prompt_time': 0.012291243, 'prompt_tokens_details': None, 'queue_time': 0.056397017, 'total_time': 1.406085901}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8e34-7bf0-a341-44b6f58b351a-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 5, 'b': 3}, 'id': '73mt5mz4w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 313, 'output_tokens': 424, 'total_tokens': 737, 'output_token_details': {'reasoning': 394}}), ToolMessage(content='8', name='add_numbers', id='3870029a-87f8-4ce6-be84-38a8f37b0136', tool_call_id='73mt5mz4w'), AIMessage(content='The sum of 5 and 3 is **8**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 5 + 3?\" and I need to respond. Let me check the previous interactions to see the pattern.\\n\\nEarlier, when they asked 3+4, I called the add_numbers function with a=3 and b=4, got 7. Then for 4+5, same function with a=4 and b=5, result 9. Then 5+3, called the function with a=5 and b=3, got 8. The user expects the sum in bold. So the response should be \"The sum of 5 and 3 is **8**.\" Just need to follow the same structure as before. Confirm the numbers and the result. Yep, 5+3 is 8. All set.\\n'}, response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 357, 'total_tokens': 536, 'completion_time': 0.347554582, 'completion_tokens_details': {'reasoning_tokens': 161}, 'prompt_time': 0.014150062, 'prompt_tokens_details': None, 'queue_time': 0.057143458, 'total_time': 0.361704644}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-946b-7923-8497-df8ab24c4697-0', usage_metadata={'input_tokens': 357, 'output_tokens': 179, 'total_tokens': 536, 'output_token_details': {'reasoning': 161}})]}\n",
      "messages : 12\n",
      "messages : {'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\n<think>\\nOkay, let\\'s see. The user wants me to extract the most important context from the conversation history to replace it and save tokens. The conversation has a few messages where the user asks addition questions, and the AI responds using a tool called add_numbers.\\n\\nFirst, the user asked \"what is 3 + 4?\" The AI used the add_numbers function with a=3 and b=4, got 7, and answered. Then the user asked \"4 + 5\", AI used the same function with a=4 and b=5, result was 9. Now the user is asking \"5 + 3\". \\n\\nThe key points here are the pattern of using the add_numbers tool for each addition problem. The important context is that the AI has been successfully using this function to compute sums. The latest question is \"5 + 3\", so the next step would be to call the function again with a=5 and b=3. \\n\\nI need to make sure the extracted context includes the function\\'s usage, the parameters, and the current question. Also, since the previous responses were straightforward, there\\'s no need to include all the token usage details or the previous answers unless they\\'re relevant. The main thing is to show that the AI is using the add_numbers function correctly and that the current query is another addition problem. That way, the conversation history is condensed but retains the necessary information for the next step.\\n</think>\\n\\nThe user has asked \"what is 5 + 3?\" following a pattern of sequential addition queries (3+4, 4+5). The AI has consistently used the `add_numbers` tool to compute results (7, 9) by passing integer parameters. The current question requires calling `add_numbers` with a=5 and b=3.', additional_kwargs={}, response_metadata={}, id='2bf56f4d-ea5f-4670-850d-baa53522520e'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 5 + 3?\" Let me see. They\\'ve previously asked similar questions like 3+4 and 4+5, and each time I used the add_numbers function. The pattern here is straightforward addition.\\n\\nFirst, I need to confirm that the function add_numbers is the right tool here. The function takes two integers, a and b, and returns their sum. The user\\'s question is 5 + 3, which are both integers. So, applying the function makes sense.\\n\\nI should check if there\\'s any reason not to use the function here. But since the user is just asking for the sum, and the function is designed for this exact purpose, it\\'s the correct approach. \\n\\nWait, in the previous interactions, the user input was in the form of \"what is X + Y?\" and I converted those into function calls. This time it\\'s \"5 + 3\", which is the same structure. No complications here. \\n\\nI need to make sure the parameters are correctly formatted. The function requires a and b as integers. 5 and 3 are integers, so that\\'s good. \\n\\nNo edge cases here. The numbers are within a reasonable range, and there\\'s no indication of any errors. Just a simple addition. \\n\\nSo, the next step is to generate the tool call with the arguments a=5 and b=3. Then, once the function returns the result, I can present the answer to the user. \\n\\nI should also remember to format the response correctly, using the JSON structure inside the tool_call tags. Let me double-check the syntax to avoid any formatting errors. \\n\\nYes, the correct JSON would be {\"name\": \"add_numbers\", \"arguments\": {\"a\": 5, \"b\": 3}}. \\n\\nAlright, that\\'s all. The function will handle the rest, and I can provide the answer once the tool responds.\\n', 'tool_calls': [{'id': '73mt5mz4w', 'function': {'arguments': '{\"a\":5,\"b\":3}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 424, 'prompt_tokens': 313, 'total_tokens': 737, 'completion_time': 1.393794658, 'completion_tokens_details': {'reasoning_tokens': 394}, 'prompt_time': 0.012291243, 'prompt_tokens_details': None, 'queue_time': 0.056397017, 'total_time': 1.406085901}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8e34-7bf0-a341-44b6f58b351a-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 5, 'b': 3}, 'id': '73mt5mz4w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 313, 'output_tokens': 424, 'total_tokens': 737, 'output_token_details': {'reasoning': 394}}), ToolMessage(content='8', name='add_numbers', id='3870029a-87f8-4ce6-be84-38a8f37b0136', tool_call_id='73mt5mz4w'), AIMessage(content='The sum of 5 and 3 is **8**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 5 + 3?\" and I need to respond. Let me check the previous interactions to see the pattern.\\n\\nEarlier, when they asked 3+4, I called the add_numbers function with a=3 and b=4, got 7. Then for 4+5, same function with a=4 and b=5, result 9. Then 5+3, called the function with a=5 and b=3, got 8. The user expects the sum in bold. So the response should be \"The sum of 5 and 3 is **8**.\" Just need to follow the same structure as before. Confirm the numbers and the result. Yep, 5+3 is 8. All set.\\n'}, response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 357, 'total_tokens': 536, 'completion_time': 0.347554582, 'completion_tokens_details': {'reasoning_tokens': 161}, 'prompt_time': 0.014150062, 'prompt_tokens_details': None, 'queue_time': 0.057143458, 'total_time': 0.361704644}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-946b-7923-8497-df8ab24c4697-0', usage_metadata={'input_tokens': 357, 'output_tokens': 179, 'total_tokens': 536, 'output_token_details': {'reasoning': 161}}), HumanMessage(content='what is 90 + 10?', additional_kwargs={}, response_metadata={}, id='bddc8f97-1772-4e98-bbe0-97fd34f92c0d'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 90 + 10?\" So I need to use the add_numbers function again. Let me check the parameters. The function requires two integers, a and b. Here, a is 90 and b is 10. Just need to make sure they\\'re integers. The previous examples had smaller numbers, but this is similar. No decimal points here, so I can pass them as integers. Let me format the tool call correctly. The response should be a JSON object inside tool_call tags. Alright, that should do it.\\n', 'tool_calls': [{'id': '2bt5djgz7', 'function': {'arguments': '{\"a\":90,\"b\":10}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 390, 'total_tokens': 541, 'completion_time': 0.258625813, 'completion_tokens_details': {'reasoning_tokens': 119}, 'prompt_time': 0.015745689, 'prompt_tokens_details': None, 'queue_time': 0.158417961, 'total_time': 0.274371502}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-967e-7460-93a7-02fd08a7f42f-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 90, 'b': 10}, 'id': '2bt5djgz7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 390, 'output_tokens': 151, 'total_tokens': 541, 'output_token_details': {'reasoning': 119}}), ToolMessage(content='100', name='add_numbers', id='476420b3-8bf5-4a9c-9edc-7779ab03f1a3', tool_call_id='2bt5djgz7'), AIMessage(content='The sum of 90 and 10 is **100**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 90 + 10?\" and I used the add_numbers tool to calculate it. The response was 100. Now, I need to present the answer clearly. Let me check if there\\'s any pattern here. The user has been asking addition questions, and each time I\\'ve used the tool correctly. The previous answers were 7, 9, and 8, and now 100. Maybe the user is testing if I can handle different numbers, including larger ones like 90 and 10. Since the tool works for all integers, I just need to state the result. I should make sure the answer is straightforward and confirms the sum. No need for extra explanation unless the user asks for it. So, the answer is 100.\\n'}, response_metadata={'token_usage': {'completion_tokens': 191, 'prompt_tokens': 657, 'total_tokens': 848, 'completion_time': 0.376262511, 'completion_tokens_details': {'reasoning_tokens': 169}, 'prompt_time': 0.027222293, 'prompt_tokens_details': None, 'queue_time': 0.051024986, 'total_time': 0.403484804}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-9d23-77e2-b17e-f4399cf9ea7a-0', usage_metadata={'input_tokens': 657, 'output_tokens': 191, 'total_tokens': 848, 'output_token_details': {'reasoning': 169}})]}\n",
      "messages : 8\n",
      "messages : {'messages': [HumanMessage(content='Here is a summary of the conversation to date:\\n\\n<think>\\nOkay, let\\'s see. The user wants me to extract the most important context from the conversation history to replace it and save tokens. The conversation has a few messages where the user asks addition questions, and the AI responds using a tool called add_numbers.\\n\\nFirst, the user asked \"what is 3 + 4?\" The AI used the add_numbers function with a=3 and b=4, got 7, and answered. Then the user asked \"4 + 5\", AI used the same function with a=4 and b=5, result was 9. Now the user is asking \"5 + 3\". \\n\\nThe key points here are the pattern of using the add_numbers tool for each addition problem. The important context is that the AI has been successfully using this function to compute sums. The latest question is \"5 + 3\", so the next step would be to call the function again with a=5 and b=3. \\n\\nI need to make sure the extracted context includes the function\\'s usage, the parameters, and the current question. Also, since the previous responses were straightforward, there\\'s no need to include all the token usage details or the previous answers unless they\\'re relevant. The main thing is to show that the AI is using the add_numbers function correctly and that the current query is another addition problem. That way, the conversation history is condensed but retains the necessary information for the next step.\\n</think>\\n\\nThe user has asked \"what is 5 + 3?\" following a pattern of sequential addition queries (3+4, 4+5). The AI has consistently used the `add_numbers` tool to compute results (7, 9) by passing integer parameters. The current question requires calling `add_numbers` with a=5 and b=3.', additional_kwargs={}, response_metadata={}, id='2bf56f4d-ea5f-4670-850d-baa53522520e'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 5 + 3?\" Let me see. They\\'ve previously asked similar questions like 3+4 and 4+5, and each time I used the add_numbers function. The pattern here is straightforward addition.\\n\\nFirst, I need to confirm that the function add_numbers is the right tool here. The function takes two integers, a and b, and returns their sum. The user\\'s question is 5 + 3, which are both integers. So, applying the function makes sense.\\n\\nI should check if there\\'s any reason not to use the function here. But since the user is just asking for the sum, and the function is designed for this exact purpose, it\\'s the correct approach. \\n\\nWait, in the previous interactions, the user input was in the form of \"what is X + Y?\" and I converted those into function calls. This time it\\'s \"5 + 3\", which is the same structure. No complications here. \\n\\nI need to make sure the parameters are correctly formatted. The function requires a and b as integers. 5 and 3 are integers, so that\\'s good. \\n\\nNo edge cases here. The numbers are within a reasonable range, and there\\'s no indication of any errors. Just a simple addition. \\n\\nSo, the next step is to generate the tool call with the arguments a=5 and b=3. Then, once the function returns the result, I can present the answer to the user. \\n\\nI should also remember to format the response correctly, using the JSON structure inside the tool_call tags. Let me double-check the syntax to avoid any formatting errors. \\n\\nYes, the correct JSON would be {\"name\": \"add_numbers\", \"arguments\": {\"a\": 5, \"b\": 3}}. \\n\\nAlright, that\\'s all. The function will handle the rest, and I can provide the answer once the tool responds.\\n', 'tool_calls': [{'id': '73mt5mz4w', 'function': {'arguments': '{\"a\":5,\"b\":3}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 424, 'prompt_tokens': 313, 'total_tokens': 737, 'completion_time': 1.393794658, 'completion_tokens_details': {'reasoning_tokens': 394}, 'prompt_time': 0.012291243, 'prompt_tokens_details': None, 'queue_time': 0.056397017, 'total_time': 1.406085901}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-8e34-7bf0-a341-44b6f58b351a-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 5, 'b': 3}, 'id': '73mt5mz4w', 'type': 'tool_call'}], usage_metadata={'input_tokens': 313, 'output_tokens': 424, 'total_tokens': 737, 'output_token_details': {'reasoning': 394}}), ToolMessage(content='8', name='add_numbers', id='3870029a-87f8-4ce6-be84-38a8f37b0136', tool_call_id='73mt5mz4w'), AIMessage(content='The sum of 5 and 3 is **8**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 5 + 3?\" and I need to respond. Let me check the previous interactions to see the pattern.\\n\\nEarlier, when they asked 3+4, I called the add_numbers function with a=3 and b=4, got 7. Then for 4+5, same function with a=4 and b=5, result 9. Then 5+3, called the function with a=5 and b=3, got 8. The user expects the sum in bold. So the response should be \"The sum of 5 and 3 is **8**.\" Just need to follow the same structure as before. Confirm the numbers and the result. Yep, 5+3 is 8. All set.\\n'}, response_metadata={'token_usage': {'completion_tokens': 179, 'prompt_tokens': 357, 'total_tokens': 536, 'completion_time': 0.347554582, 'completion_tokens_details': {'reasoning_tokens': 161}, 'prompt_time': 0.014150062, 'prompt_tokens_details': None, 'queue_time': 0.057143458, 'total_time': 0.361704644}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-946b-7923-8497-df8ab24c4697-0', usage_metadata={'input_tokens': 357, 'output_tokens': 179, 'total_tokens': 536, 'output_token_details': {'reasoning': 161}}), HumanMessage(content='what is 90 + 10?', additional_kwargs={}, response_metadata={}, id='bddc8f97-1772-4e98-bbe0-97fd34f92c0d'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 90 + 10?\" So I need to use the add_numbers function again. Let me check the parameters. The function requires two integers, a and b. Here, a is 90 and b is 10. Just need to make sure they\\'re integers. The previous examples had smaller numbers, but this is similar. No decimal points here, so I can pass them as integers. Let me format the tool call correctly. The response should be a JSON object inside tool_call tags. Alright, that should do it.\\n', 'tool_calls': [{'id': '2bt5djgz7', 'function': {'arguments': '{\"a\":90,\"b\":10}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 151, 'prompt_tokens': 390, 'total_tokens': 541, 'completion_time': 0.258625813, 'completion_tokens_details': {'reasoning_tokens': 119}, 'prompt_time': 0.015745689, 'prompt_tokens_details': None, 'queue_time': 0.158417961, 'total_time': 0.274371502}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-967e-7460-93a7-02fd08a7f42f-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 90, 'b': 10}, 'id': '2bt5djgz7', 'type': 'tool_call'}], usage_metadata={'input_tokens': 390, 'output_tokens': 151, 'total_tokens': 541, 'output_token_details': {'reasoning': 119}}), ToolMessage(content='100', name='add_numbers', id='476420b3-8bf5-4a9c-9edc-7779ab03f1a3', tool_call_id='2bt5djgz7'), AIMessage(content='The sum of 90 and 10 is **100**.', additional_kwargs={'reasoning_content': 'Okay, the user asked \"what is 90 + 10?\" and I used the add_numbers tool to calculate it. The response was 100. Now, I need to present the answer clearly. Let me check if there\\'s any pattern here. The user has been asking addition questions, and each time I\\'ve used the tool correctly. The previous answers were 7, 9, and 8, and now 100. Maybe the user is testing if I can handle different numbers, including larger ones like 90 and 10. Since the tool works for all integers, I just need to state the result. I should make sure the answer is straightforward and confirms the sum. No need for extra explanation unless the user asks for it. So, the answer is 100.\\n'}, response_metadata={'token_usage': {'completion_tokens': 191, 'prompt_tokens': 657, 'total_tokens': 848, 'completion_time': 0.376262511, 'completion_tokens_details': {'reasoning_tokens': 169}, 'prompt_time': 0.027222293, 'prompt_tokens_details': None, 'queue_time': 0.051024986, 'total_time': 0.403484804}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-9d23-77e2-b17e-f4399cf9ea7a-0', usage_metadata={'input_tokens': 657, 'output_tokens': 191, 'total_tokens': 848, 'output_token_details': {'reasoning': 169}}), HumanMessage(content='what is 10 + 90?', additional_kwargs={}, response_metadata={}, id='21fad4e5-37bf-454d-af92-810eede9b7bf'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user is asking \"what is 10 + 90?\" now. Let me check the conversation history. They\\'ve been doing a series of addition problems: 3+4, 4+5, 5+3, 90+10, and now 10+90. The AI has used the add_numbers tool each time, correctly adding the numbers. \\n\\nHmm, 10 + 90 is the same as 90 + 10, which was answered earlier as 100. The user might be testing if the AI recognizes the commutative property of addition. Even though the order is reversed, the sum remains the same. \\n\\nI should call the add_numbers function again with a=10 and b=90. The tool will return 100, and then I can explain that the order doesn\\'t affect the result. That way, the user knows the AI understands that addition is commutative.\\n', 'tool_calls': [{'id': 'nggwqte4z', 'function': {'arguments': '{\"a\":10,\"b\":90}', 'name': 'add_numbers'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 240, 'prompt_tokens': 694, 'total_tokens': 934, 'completion_time': 0.449210529, 'completion_tokens_details': {'reasoning_tokens': 204}, 'prompt_time': 0.030872485, 'prompt_tokens_details': None, 'queue_time': 0.057059275, 'total_time': 0.480083014}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-a3c3-7be0-ba24-42ec5bae529e-0', tool_calls=[{'name': 'add_numbers', 'args': {'a': 10, 'b': 90}, 'id': 'nggwqte4z', 'type': 'tool_call'}], usage_metadata={'input_tokens': 694, 'output_tokens': 240, 'total_tokens': 934, 'output_token_details': {'reasoning': 204}}), ToolMessage(content='100', name='add_numbers', id='7635b3ef-8d01-4390-9391-1dc806ad9c40', tool_call_id='nggwqte4z'), AIMessage(content='The sum of 10 and 90 is **100**.', additional_kwargs={'reasoning_content': \"Okay, let's see. The user has been asking addition questions so far. First, they asked 3 + 4, then 4 + 5, then 5 + 3, 90 + 10, and now 10 + 90. Each time, I used the add_numbers function to get the result. The pattern here is that the user is testing different combinations of numbers, including reversing the order of the same numbers, like 90 + 10 and then 10 + 90.\\n\\nI need to check if there's any trick here. Since addition is commutative, the order shouldn't matter. The function should return the same result regardless of the order. So for 10 + 90, the answer should still be 100. The user might be verifying if the tool works correctly even when the operands are swapped. \\n\\nI should make sure that the function is called correctly with the given parameters. The previous responses were correct, so following the same process should work here. No need for any extra steps here. Just call the add_numbers function with a=10 and b=90, which should sum to 100 again. The user probably expects confirmation that the order doesn't affect the result, so the answer is straightforward.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 294, 'prompt_tokens': 742, 'total_tokens': 1036, 'completion_time': 0.588903518, 'completion_tokens_details': {'reasoning_tokens': 272}, 'prompt_time': 0.031566296, 'prompt_tokens_details': None, 'queue_time': 0.158721833, 'total_time': 0.620469814}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6afb-c275-7491-9949-5826d92f5f82-0', usage_metadata={'input_tokens': 742, 'output_tokens': 294, 'total_tokens': 1036, 'output_token_details': {'reasoning': 272}})]}\n",
      "messages : 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage , HumanMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import os\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0.2  \n",
    ")\n",
    "\n",
    "# 2) Middleware\n",
    "summarization = SummarizationMiddleware(\n",
    "    model=llm,\n",
    "    trigger=(\"messages\",15),\n",
    "    keep=(\"messages\", 6)\n",
    ")\n",
    "\n",
    "# 3) Tools (example)\n",
    "@tool \n",
    "def add_numbers(a: int, b: int) -> str:\n",
    "    \"\"\"Add two numbers\"\"\"\n",
    "    return str(a + b)\n",
    "\n",
    "tools = [add_numbers]\n",
    "\n",
    "# 4) Initialize the agent\n",
    "agent = create_agent(\n",
    "    tools=tools,\n",
    "    model=llm,\n",
    "    checkpointer = InMemorySaver(),\n",
    "    middleware=[summarization]\n",
    ")\n",
    "questions = [\n",
    "    \"what is 3 + 4?\",\n",
    "    \"what is 4 + 5?\",\n",
    "    \"what is 5 + 3?\",\n",
    "    \"what is 90 + 10?\",\n",
    "    \"what is 10 + 90?\",\n",
    "    \"what is 50 + 50?\",\n",
    "    \"what is 100 + 100?\"\n",
    "    \"what is 3 + 10?\",\n",
    "    \"what is 4 + 10?\",\n",
    "    \"what is 5 + 10?\",\n",
    "    \"what is 90 + 100?\",\n",
    "    \"what is 10 + 100?\",\n",
    "    \"what is 50 + 100?\",\n",
    "    \"what is 100 + 100?\"\n",
    "\n",
    "]\n",
    "config ={'configurable':{'thread_id':'test-1'}}\n",
    "for q in questions:\n",
    "    res = agent.invoke({\"messages\" : HumanMessage(content=q)},config)\n",
    "    print(f\"messages : {res}\")\n",
    "    print(f\"messages : { len(res['messages'])}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db580ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: New York, Tokens: 128 length: 4\n",
      "response: [HumanMessage(content='New York', additional_kwargs={}, response_metadata={}, id='582fe4ff-7949-484b-936e-ec4d01049ce6'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user mentioned \"New York\". Let me check the available tools. There\\'s a function called search_hotel that requires a city parameter. Since the user specified New York, I should call that function with the city set to New York. I need to make sure the arguments are correctly formatted as JSON within the tool_call tags. Let me structure the response properly.\\n', 'tool_calls': [{'id': 't6vye5s73', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'search_hotel'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 150, 'total_tokens': 251, 'completion_time': 0.169354767, 'completion_tokens_details': {'reasoning_tokens': 75}, 'prompt_time': 0.006900757, 'prompt_tokens_details': None, 'queue_time': 0.057144823, 'total_time': 0.176255524}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6b39-1c97-7b70-b942-8558660b1148-0', tool_calls=[{'name': 'search_hotel', 'args': {'city': 'New York'}, 'id': 't6vye5s73', 'type': 'tool_call'}], usage_metadata={'input_tokens': 150, 'output_tokens': 101, 'total_tokens': 251, 'output_token_details': {'reasoning': 75}}), ToolMessage(content='Searching for hotels in New York\\n    1. Hotel A with star rating 4 rent 20$\\n    2. Hotel B with star rating 5 rent 30$\\n    3. Hotel C with star rating 3 rent 10$\\n    ', name='search_hotel', id='347ae503-ecc0-4216-90bb-41c81357794c', tool_call_id='t6vye5s73'), AIMessage(content='Here are the hotel options in New York based on your search:\\n\\n1. **Hotel A**  \\n   - Star rating: 4  \\n   - Price: $20/night  \\n\\n2. **Hotel B**  \\n   - Star rating: 5  \\n   - Price: $30/night  \\n\\n3. **Hotel C**  \\n   - Star rating: 3  \\n   - Price: $10/night  \\n\\nWould you like assistance with booking or more details about any of these options?', additional_kwargs={'reasoning_content': 'Okay, let\\'s see. The user asked for hotels in New York, and I called the search_hotel function with the city parameter. The response listed three hotels with their ratings and prices. Now I need to present this information clearly.\\n\\nFirst, I should summarize the options. Maybe start by stating that I found hotels in New York. Then list each one with their details. I should mention the star rating and price for each. Let me check the data again to make sure I have it right. Hotel A is 4 stars at $20, Hotel B is 5 stars at $30, and Hotel C is 3 stars at $10. \\n\\nWait, the prices seem a bit low. Is that in USD? The user didn\\'t specify a currency, but since it\\'s New York, it\\'s safe to assume dollars. Should I mention the currency? The original response didn\\'t include it, but maybe adding \"$\" would make it clearer. Also, the star ratings are important for quality, so highlighting those makes sense. \\n\\nI should structure the response in a way that\\'s easy to read. Maybe bullet points or a numbered list. The user might want to choose based on price or rating. Including both factors helps them decide. Let me make sure each hotel\\'s info is clear. \\n\\nIs there any additional info needed? The original response didn\\'t include amenities or locations, so I can\\'t provide more details. I should stick to what\\'s given. Maybe end with a question to see if they want to book or need more info. That way, the user feels supported in the next step. \\n\\nDouble-checking for typos: the hotel names are A, B, C as per the response. Prices are 20, 30, 10. Ratings 4,5,3. All correct. Alright, time to put it all together in a friendly, helpful manner.\\n'}, response_metadata={'token_usage': {'completion_tokens': 501, 'prompt_tokens': 240, 'total_tokens': 741, 'completion_time': 1.5326480839999999, 'completion_tokens_details': {'reasoning_tokens': 390}, 'prompt_time': 0.010686424, 'prompt_tokens_details': None, 'queue_time': 0.056419916, 'total_time': 1.543334508}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6b39-1e43-76f3-aa59-d7ae7eef57cf-0', usage_metadata={'input_tokens': 240, 'output_tokens': 501, 'total_tokens': 741, 'output_token_details': {'reasoning': 390}})]\n",
      "City: Los Angeles, Tokens: 258 length: 8\n",
      "response: [HumanMessage(content='New York', additional_kwargs={}, response_metadata={}, id='582fe4ff-7949-484b-936e-ec4d01049ce6'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user mentioned \"New York\". Let me check the available tools. There\\'s a function called search_hotel that requires a city parameter. Since the user specified New York, I should call that function with the city set to New York. I need to make sure the arguments are correctly formatted as JSON within the tool_call tags. Let me structure the response properly.\\n', 'tool_calls': [{'id': 't6vye5s73', 'function': {'arguments': '{\"city\":\"New York\"}', 'name': 'search_hotel'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 150, 'total_tokens': 251, 'completion_time': 0.169354767, 'completion_tokens_details': {'reasoning_tokens': 75}, 'prompt_time': 0.006900757, 'prompt_tokens_details': None, 'queue_time': 0.057144823, 'total_time': 0.176255524}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6b39-1c97-7b70-b942-8558660b1148-0', tool_calls=[{'name': 'search_hotel', 'args': {'city': 'New York'}, 'id': 't6vye5s73', 'type': 'tool_call'}], usage_metadata={'input_tokens': 150, 'output_tokens': 101, 'total_tokens': 251, 'output_token_details': {'reasoning': 75}}), ToolMessage(content='Searching for hotels in New York\\n    1. Hotel A with star rating 4 rent 20$\\n    2. Hotel B with star rating 5 rent 30$\\n    3. Hotel C with star rating 3 rent 10$\\n    ', name='search_hotel', id='347ae503-ecc0-4216-90bb-41c81357794c', tool_call_id='t6vye5s73'), AIMessage(content='Here are the hotel options in New York based on your search:\\n\\n1. **Hotel A**  \\n   - Star rating: 4  \\n   - Price: $20/night  \\n\\n2. **Hotel B**  \\n   - Star rating: 5  \\n   - Price: $30/night  \\n\\n3. **Hotel C**  \\n   - Star rating: 3  \\n   - Price: $10/night  \\n\\nWould you like assistance with booking or more details about any of these options?', additional_kwargs={'reasoning_content': 'Okay, let\\'s see. The user asked for hotels in New York, and I called the search_hotel function with the city parameter. The response listed three hotels with their ratings and prices. Now I need to present this information clearly.\\n\\nFirst, I should summarize the options. Maybe start by stating that I found hotels in New York. Then list each one with their details. I should mention the star rating and price for each. Let me check the data again to make sure I have it right. Hotel A is 4 stars at $20, Hotel B is 5 stars at $30, and Hotel C is 3 stars at $10. \\n\\nWait, the prices seem a bit low. Is that in USD? The user didn\\'t specify a currency, but since it\\'s New York, it\\'s safe to assume dollars. Should I mention the currency? The original response didn\\'t include it, but maybe adding \"$\" would make it clearer. Also, the star ratings are important for quality, so highlighting those makes sense. \\n\\nI should structure the response in a way that\\'s easy to read. Maybe bullet points or a numbered list. The user might want to choose based on price or rating. Including both factors helps them decide. Let me make sure each hotel\\'s info is clear. \\n\\nIs there any additional info needed? The original response didn\\'t include amenities or locations, so I can\\'t provide more details. I should stick to what\\'s given. Maybe end with a question to see if they want to book or need more info. That way, the user feels supported in the next step. \\n\\nDouble-checking for typos: the hotel names are A, B, C as per the response. Prices are 20, 30, 10. Ratings 4,5,3. All correct. Alright, time to put it all together in a friendly, helpful manner.\\n'}, response_metadata={'token_usage': {'completion_tokens': 501, 'prompt_tokens': 240, 'total_tokens': 741, 'completion_time': 1.5326480839999999, 'completion_tokens_details': {'reasoning_tokens': 390}, 'prompt_time': 0.010686424, 'prompt_tokens_details': None, 'queue_time': 0.056419916, 'total_time': 1.543334508}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6b39-1e43-76f3-aa59-d7ae7eef57cf-0', usage_metadata={'input_tokens': 240, 'output_tokens': 501, 'total_tokens': 741, 'output_token_details': {'reasoning': 390}}), HumanMessage(content='Los Angeles', additional_kwargs={}, response_metadata={}, id='6cd75185-5adb-4cb3-bbc3-d535b7e868a5'), AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user previously asked about hotels in New York, and I provided a list. Now they\\'re asking about Los Angeles. I need to use the same search_hotel function again. Let me check the function parameters. The function requires the city name, so I should call search_hotel with \"Los Angeles\" as the city parameter. I need to make sure the JSON is correctly formatted and that the city name is in quotes. No other parameters are needed since the function only requires the city. Let me structure the tool_call properly.\\n', 'tool_calls': [{'id': 'jxhe7b70v', 'function': {'arguments': '{\"city\":\"Los Angeles\"}', 'name': 'search_hotel'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 358, 'total_tokens': 493, 'completion_time': 0.251501027, 'completion_tokens_details': {'reasoning_tokens': 109}, 'prompt_time': 0.014359254, 'prompt_tokens_details': None, 'queue_time': 0.050602556, 'total_time': 0.265860281}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6b39-2503-75f1-9340-b516dca5bb42-0', tool_calls=[{'name': 'search_hotel', 'args': {'city': 'Los Angeles'}, 'id': 'jxhe7b70v', 'type': 'tool_call'}], usage_metadata={'input_tokens': 358, 'output_tokens': 135, 'total_tokens': 493, 'output_token_details': {'reasoning': 109}}), ToolMessage(content='Searching for hotels in Los Angeles\\n    1. Hotel A with star rating 4 rent 20$\\n    2. Hotel B with star rating 5 rent 30$\\n    3. Hotel C with star rating 3 rent 10$\\n    ', name='search_hotel', id='3e90a3bd-41c3-4163-adb9-d9e7b5e1083b', tool_call_id='jxhe7b70v'), AIMessage(content='Here are the hotel options in Los Angeles based on your search:\\n\\n1. **Hotel A**  \\n   - Star rating: 4  \\n   - Price: $20/night  \\n\\n2. **Hotel B**  \\n   - Star rating: 5  \\n   - Price: $30/night  \\n\\n3. **Hotel C**  \\n   - Star rating: 3  \\n   - Price: $10/night  \\n\\nWould you like assistance with booking or more details about any of these options?', additional_kwargs={'reasoning_content': \"Okay, the user asked for Los Angeles hotels, and the tool response has the same structure as the previous New York search. Let me check the hotels listed. The options are Hotel A (4, $20), Hotel B (5, $30), and Hotel C (3, $10). The user might be looking for a place to stay and comparing prices and ratings.\\n\\nSince the user hasn't specified any particular preferences yet, I should present the options clearly. Maybe they want a balance between price and rating. Hotel A offers a good middle ground, while Hotel B is the most expensive but highest rated. Hotel C is the cheapest but lowest in stars. \\n\\nI should list them out in a friendly manner, highlighting the key details. Also, offer further assistance in case they need more info or help booking. Keep the response concise but helpful. Let me make sure the formatting is consistent with the previous response so it's easy to read.\\n\"}, response_metadata={'token_usage': {'completion_tokens': 307, 'prompt_tokens': 448, 'total_tokens': 755, 'completion_time': 0.55329279, 'completion_tokens_details': {'reasoning_tokens': 196}, 'prompt_time': 0.018395484, 'prompt_tokens_details': None, 'queue_time': 0.159089995, 'total_time': 0.571688274}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6b39-26bb-77d3-95e5-e07d642c277a-0', usage_metadata={'input_tokens': 448, 'output_tokens': 307, 'total_tokens': 755, 'output_token_details': {'reasoning': 196}})]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPStatusError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1024\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1025\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.HTTPStatusError \u001b[38;5;28;01mas\u001b[39;00m err:  \u001b[38;5;66;03m# thrown on 4xx and 5xx status code\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\httpx\\_models.py:829\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    828\u001b[39m message = message.format(\u001b[38;5;28mself\u001b[39m, error_type=error_type)\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m HTTPStatusError(message, request=request, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPStatusError\u001b[39m: Client error '429 Too Many Requests' for url 'https://api.groq.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m cities = [\u001b[33m'\u001b[39m\u001b[33mNew York\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mLos Angeles\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mChicago\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mHouston\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mPhoenix\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m cities:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     res = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcity\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m     token = token_count(res[\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     62\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCity: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Tokens: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtoken\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m length: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(res[\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:3068\u001b[39m, in \u001b[36mPregel.invoke\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[39m\n\u001b[32m   3065\u001b[39m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] | Any] = []\n\u001b[32m   3066\u001b[39m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] = []\n\u001b[32m-> \u001b[39m\u001b[32m3068\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3069\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3070\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3071\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3072\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mupdates\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m   3073\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m   3074\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3075\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3076\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3077\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3078\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3079\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdurability\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3080\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3081\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   3082\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvalues\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m   3083\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\main.py:2643\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2641\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2642\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2643\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2644\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2645\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2646\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2647\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2648\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2650\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2652\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2653\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_runner.py:167\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    165\u001b[39m t = tasks[\u001b[32m0\u001b[39m]\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    168\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m=\u001b[49m\u001b[43mweakref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28mself\u001b[39m.commit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langgraph\\_internal\\_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain\\agents\\middleware\\summarization.py:283\u001b[39m, in \u001b[36mSummarizationMiddleware.before_model\u001b[39m\u001b[34m(self, state, runtime)\u001b[39m\n\u001b[32m    279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    281\u001b[39m messages_to_summarize, preserved_messages = \u001b[38;5;28mself\u001b[39m._partition_messages(messages, cutoff_index)\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m summary = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages_to_summarize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m new_messages = \u001b[38;5;28mself\u001b[39m._build_new_messages(summary)\n\u001b[32m    286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    287\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m    288\u001b[39m         RemoveMessage(\u001b[38;5;28mid\u001b[39m=REMOVE_ALL_MESSAGES),\n\u001b[32m   (...)\u001b[39m\u001b[32m    291\u001b[39m     ]\n\u001b[32m    292\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain\\agents\\middleware\\summarization.py:495\u001b[39m, in \u001b[36mSummarizationMiddleware._create_summary\u001b[39m\u001b[34m(self, messages_to_summarize)\u001b[39m\n\u001b[32m    492\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mPrevious conversation was too long to summarize.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msummary_prompt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrimmed_messages\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response.text.strip()\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:398\u001b[39m, in \u001b[36mBaseChatModel.invoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    384\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    386\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     **kwargs: Any,\n\u001b[32m    392\u001b[39m ) -> AIMessage:\n\u001b[32m    393\u001b[39m     config = ensure_config(config)\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    395\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    396\u001b[39m         cast(\n\u001b[32m    397\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m--> \u001b[39m\u001b[32m398\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    400\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcallbacks\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    402\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtags\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    403\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    404\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_name\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    405\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_id\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    406\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    407\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m],\n\u001b[32m    408\u001b[39m         ).message,\n\u001b[32m    409\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1117\u001b[39m, in \u001b[36mBaseChatModel.generate_prompt\u001b[39m\u001b[34m(self, prompts, stop, callbacks, **kwargs)\u001b[39m\n\u001b[32m   1108\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgenerate_prompt\u001b[39m(\n\u001b[32m   1110\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1114\u001b[39m     **kwargs: Any,\n\u001b[32m   1115\u001b[39m ) -> LLMResult:\n\u001b[32m   1116\u001b[39m     prompt_messages = [p.to_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[32m-> \u001b[39m\u001b[32m1117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:927\u001b[39m, in \u001b[36mBaseChatModel.generate\u001b[39m\u001b[34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[39m\n\u001b[32m    924\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    926\u001b[39m         results.append(\n\u001b[32m--> \u001b[39m\u001b[32m927\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m                \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m         )\n\u001b[32m    934\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    935\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1221\u001b[39m, in \u001b[36mBaseChatModel._generate_with_cache\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m   1219\u001b[39m     result = generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[32m   1220\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m inspect.signature(\u001b[38;5;28mself\u001b[39m._generate).parameters.get(\u001b[33m\"\u001b[39m\u001b[33mrun_manager\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1221\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1222\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m   1223\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1225\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._generate(messages, stop=stop, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\langchain_groq\\chat_models.py:593\u001b[39m, in \u001b[36mChatGroq._generate\u001b[39m\u001b[34m(self, messages, stop, run_manager, **kwargs)\u001b[39m\n\u001b[32m    588\u001b[39m message_dicts, params = \u001b[38;5;28mself\u001b[39m._create_message_dicts(messages, stop)\n\u001b[32m    589\u001b[39m params = {\n\u001b[32m    590\u001b[39m     **params,\n\u001b[32m    591\u001b[39m     **kwargs,\n\u001b[32m    592\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmessage_dicts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._create_chat_result(response, params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\groq\\resources\\chat\\completions.py:461\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, citation_options, compound_custom, disable_tool_validation, documents, exclude_domains, frequency_penalty, function_call, functions, include_domains, include_reasoning, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, n, parallel_tool_calls, presence_penalty, reasoning_effort, reasoning_format, response_format, search_settings, seed, service_tier, stop, store, stream, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m    241\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m    242\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    243\u001b[39m     *,\n\u001b[32m   (...)\u001b[39m\u001b[32m    300\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m    301\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m    302\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    303\u001b[39m \u001b[33;03m    Creates a model response for the given chat conversation.\u001b[39;00m\n\u001b[32m    304\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    459\u001b[39m \u001b[33;03m      timeout: Override the client-level default timeout for this request, in seconds\u001b[39;00m\n\u001b[32m    460\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m461\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    462\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/openai/v1/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    463\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    465\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    466\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    467\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcitation_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcitation_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    468\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompound_custom\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompound_custom\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdisable_tool_validation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisable_tool_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    470\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdocuments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    471\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mexclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    472\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    473\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    475\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_domains\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_domains\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    476\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_reasoning\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_reasoning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    477\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    478\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    479\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    481\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    482\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    483\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    484\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    485\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    486\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    487\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msearch_settings\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msearch_settings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1242\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1228\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1229\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1230\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1237\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1238\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1239\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1240\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1241\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1242\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1030\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1028\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m remaining_retries > \u001b[32m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._should_retry(err.response):\n\u001b[32m   1029\u001b[39m     err.response.close()\n\u001b[32m-> \u001b[39m\u001b[32m1030\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sleep_for_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1032\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1033\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1034\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1035\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1036\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1038\u001b[39m \u001b[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001b[39;00m\n\u001b[32m   1039\u001b[39m \u001b[38;5;66;03m# to completion before attempting to access the response text.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\Langchain_Agent\\.venv\\Lib\\site-packages\\groq\\_base_client.py:1070\u001b[39m, in \u001b[36mSyncAPIClient._sleep_for_retry\u001b[39m\u001b[34m(self, retries_taken, max_retries, options, response)\u001b[39m\n\u001b[32m   1067\u001b[39m timeout = \u001b[38;5;28mself\u001b[39m._calculate_retry_timeout(remaining_retries, options, response.headers \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1068\u001b[39m log.info(\u001b[33m\"\u001b[39m\u001b[33mRetrying request to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m in \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m, options.url, timeout)\n\u001b[32m-> \u001b[39m\u001b[32m1070\u001b[39m \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain.agents.middleware import SummarizationMiddleware\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage , HumanMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0.2  \n",
    ")\n",
    "\n",
    "# 2) Middleware\n",
    "summarization = SummarizationMiddleware(\n",
    "    model=llm,\n",
    "    trigger=(\"tokens\",300),\n",
    "    keep=(\"tokens\", 60)\n",
    ")\n",
    "\n",
    "# 3) Tools (example)\n",
    "@tool\n",
    "def search_hotel(city:str)->str:\n",
    "    \"\"\"Search for hotels in a given city\"\"\"\n",
    "    return f\"\"\"Searching for hotels in {city}\n",
    "    1. Hotel A with star rating 4 rent 20$\n",
    "    2. Hotel B with star rating 5 rent 30$\n",
    "    3. Hotel C with star rating 3 rent 10$\n",
    "    \"\"\"\n",
    "\n",
    "tools = [search_hotel]\n",
    "\n",
    "# 4) Initialize the agent\n",
    "agent = create_agent(\n",
    "    tools=tools,\n",
    "    model=llm,\n",
    "    checkpointer = InMemorySaver(),\n",
    "    middleware=[summarization]\n",
    ")\n",
    "\n",
    "config ={'configurable':{'thread_id':'test-1'}}\n",
    "def token_count(messages:str)->int:\n",
    "    total_tokens =sum(len(str(m.content)) for m in messages)\n",
    "    return total_tokens //4\n",
    "\n",
    "cities = ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix']\n",
    "for city in cities:\n",
    "    res = agent.invoke({\"messages\" : HumanMessage(content=city)},config)\n",
    "    token = token_count(res[\"messages\"])\n",
    "    print(f\"City: {city}, Tokens: {token} length: {len(res['messages'])}\")\n",
    "    print(f\"response: {res['messages']}\")\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4595a023",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paused. Approving...\n",
      "Result: The email has been successfully sent to test@example.com with the subject \"test\" and body \"testsome\". Let me know if you need anything else!\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage , HumanMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.types import Command\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0.2  \n",
    ")\n",
    "\n",
    "\n",
    "# 3) Tools (example)\n",
    "\n",
    "def read_mail_tool(email_id:str)->str:\n",
    "    \"\"\"Read an email and return the content\"\"\"\n",
    "    return f\" read the content of email with id {email_id}\"\n",
    "\n",
    "def send_mail_tool(recipent:str , subject:str , body:str)->str:\n",
    "    \"\"\"Send an email and return the content\"\"\"\n",
    "    return f\" send recipient to {recipent} with subject {subject} and body {body}\"\n",
    "\n",
    "\n",
    "# 4) Initialize the agent\n",
    "agent = create_agent(\n",
    "    tools=[read_mail_tool , send_mail_tool],\n",
    "    model=llm,\n",
    "    checkpointer = InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_mail_tool\":\n",
    "                    {\n",
    "                        \"allowed_decisions\":[\"approve\",\"edit\",\"reject\"],\n",
    "\n",
    "                    },\n",
    "                    \"read_mail_tool\": False\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "config ={'configurable':{'thread_id':'test-2'}}\n",
    "result = agent.invoke({\"messages\" : HumanMessage(content=\"send email to test@example.com with subject test and body testsome\")},config)\n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"Paused. Approving...\")\n",
    "    resoo = agent.invoke(Command(\n",
    "        resume ={\n",
    "            \"decisions\":[{'type':'approve'}]\n",
    "        }\n",
    "    ),config=config)\n",
    "    print(f\"Result: {resoo['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2594eb42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "709c0c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paused. Approving...\n",
      "Result: It seems the tool call to send the email was rejected. Would you like to try sending the email again or is there another action you'd like to take? Let me know how I can assist!\n"
     ]
    }
   ],
   "source": [
    "config ={'configurable':{'thread_id':'test-2'}}\n",
    "result = agent.invoke({\"messages\" : HumanMessage(content=\"send email to test@example.com with subject test and body testsome\")},config)\n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"Paused. Approving...\")\n",
    "    resoore = agent.invoke(Command(\n",
    "        resume ={\n",
    "            \"decisions\":[{'type':'reject'}]\n",
    "        }\n",
    "    ),config=config)\n",
    "    print(f\"Result: {resoore['messages'][-1].content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "11956d8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='send email to test@example.com with subject test and body testsome', additional_kwargs={}, response_metadata={}, id='82ee0937-2394-4065-b111-6241c8f8266f'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, let\\'s see. The user wants to send an email to test@example.com with the subject \"test\" and the body \"testsome\". I need to use the send_mail_tool for this. The parameters required are recipent, subject, and body. The recipent is test@example.com, subject is test, and body is testsome. I should structure the tool call with these arguments. Let me double-check the parameters to make sure I didn\\'t miss any. Yep, all required fields are there. So the JSON object should have the name as send_mail_tool and the arguments with those values. I need to make sure the JSON is correctly formatted without any typos. Alright, that should do it.\\n', 'tool_calls': [{'id': '63fw89j35', 'function': {'arguments': '{\"body\":\"testsome\",\"recipent\":\"test@example.com\",\"subject\":\"test\"}', 'name': 'send_mail_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 187, 'prompt_tokens': 249, 'total_tokens': 436, 'completion_time': 0.291375128, 'completion_tokens_details': {'reasoning_tokens': 145}, 'prompt_time': 0.010247408, 'prompt_tokens_details': None, 'queue_time': 0.056263292, 'total_time': 0.301622536}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6ba1-9147-7300-8c3a-92bc9744c864-0', tool_calls=[{'name': 'send_mail_tool', 'args': {'body': 'testsome', 'recipent': 'test@example.com', 'subject': 'test'}, 'id': '63fw89j35', 'type': 'tool_call'}], usage_metadata={'input_tokens': 249, 'output_tokens': 187, 'total_tokens': 436, 'output_token_details': {'reasoning': 145}}),\n",
       "  ToolMessage(content=' send recipient to test@example.com with subject test and body testsome', name='send_mail_tool', id='036ed1f3-0dc3-48fa-b2d1-137f1fd58f8d', tool_call_id='63fw89j35'),\n",
       "  AIMessage(content='The email has been successfully sent to test@example.com with the subject \"test\" and body \"testsome\". Let me know if you need anything else!', additional_kwargs={'reasoning_content': 'Okay, the user asked to send an email to test@example.com with subject \"test\" and body \"testsome\". I used the send_mail_tool with the provided parameters. The tool response says the email was sent successfully. Now I need to confirm to the user that the email was sent. I should keep it concise and let them know it\\'s done. Maybe add a friendly note asking if they need anything else. Make sure there\\'s no markdown and the response is natural.\\n'}, response_metadata={'token_usage': {'completion_tokens': 132, 'prompt_tokens': 313, 'total_tokens': 445, 'completion_time': 0.213479404, 'completion_tokens_details': {'reasoning_tokens': 96}, 'prompt_time': 0.012875186, 'prompt_tokens_details': None, 'queue_time': 0.055987194, 'total_time': 0.22635459}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6ba1-933d-7352-8b4e-73308855ce3f-0', usage_metadata={'input_tokens': 313, 'output_tokens': 132, 'total_tokens': 445, 'output_token_details': {'reasoning': 96}}),\n",
       "  HumanMessage(content='send email to test@example.com with subject test and body testsome', additional_kwargs={}, response_metadata={}, id='962a3a6c-d984-438a-9100-fe316db57456'),\n",
       "  AIMessage(content='', additional_kwargs={'reasoning_content': 'Okay, the user wants to send another email to test@example.com with the same subject and body. Let me check the tools available. There\\'s the send_mail_tool function. The parameters required are recipent, subject, and body. The user provided all three: recipent is test@example.com, subject is \"test\", and body is \"testsome\". I need to make sure the arguments are correctly formatted. The previous response used the send_mail_tool with these parameters, so I can replicate that. Just need to structure the JSON correctly within the tool_call tags. Let me double-check the parameters for any typos. Everything looks good. Alright, time to generate the tool_call.\\n', 'tool_calls': [{'id': 'agkdxcgw0', 'function': {'arguments': '{\"body\":\"testsome\",\"recipent\":\"test@example.com\",\"subject\":\"test\"}', 'name': 'send_mail_tool'}, 'type': 'function'}]}, response_metadata={'token_usage': {'completion_tokens': 180, 'prompt_tokens': 367, 'total_tokens': 547, 'completion_time': 0.306824994, 'completion_tokens_details': {'reasoning_tokens': 138}, 'prompt_time': 0.01897715, 'prompt_tokens_details': None, 'queue_time': 0.05879252, 'total_time': 0.325802144}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_5cf921caa2', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019b6ba9-2365-7ac1-9d04-49a4dc3735a3-0', tool_calls=[{'name': 'send_mail_tool', 'args': {'body': 'testsome', 'recipent': 'test@example.com', 'subject': 'test'}, 'id': 'agkdxcgw0', 'type': 'tool_call'}], usage_metadata={'input_tokens': 367, 'output_tokens': 180, 'total_tokens': 547, 'output_token_details': {'reasoning': 138}})],\n",
       " '__interrupt__': [Interrupt(value={'action_requests': [{'name': 'send_mail_tool', 'args': {'body': 'testsome', 'recipent': 'test@example.com', 'subject': 'test'}, 'description': \"Tool execution requires approval\\n\\nTool: send_mail_tool\\nArgs: {'body': 'testsome', 'recipent': 'test@example.com', 'subject': 'test'}\"}], 'review_configs': [{'action_name': 'send_mail_tool', 'allowed_decisions': ['approve', 'edit', 'reject']}]}, id='e85bf96780d7927e689d1cf98c21790d')]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b88b04c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paused. Approving...\n",
      "Result: It looks like there was a mistake in the email address and the content. The email was sent to **abct@example.com** with subject **testabc** and body **testsomeabc**, but you originally intended to send it to **test@example.com** with subject **test** and body **testsome**.\n",
      "\n",
      "Would you like me to correct the details and resend the email?\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.tools import tool\n",
    "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import BaseMessage , HumanMessage\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langchain_groq import ChatGroq\n",
    "from dotenv import load_dotenv\n",
    "from langgraph.types import Command\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"qwen/qwen3-32b\",\n",
    "    temperature=0.2  \n",
    ")\n",
    "\n",
    "\n",
    "# 3) Tools (example)\n",
    "\n",
    "def read_mail_tool(email_id:str)->str:\n",
    "    \"\"\"Read an email and return the content\"\"\"\n",
    "    return f\" read the content of email with id {email_id}\"\n",
    "\n",
    "def send_mail_tool(recipent:str , subject:str , body:str)->str:\n",
    "    \"\"\"Send an email and return the content\"\"\"\n",
    "    return f\" send recipient to {recipent} with subject {subject} and body {body}\"\n",
    "\n",
    "\n",
    "# 4) Initialize the agent\n",
    "agent = create_agent(\n",
    "    tools=[read_mail_tool , send_mail_tool],\n",
    "    model=llm,\n",
    "    checkpointer = InMemorySaver(),\n",
    "    middleware=[\n",
    "        HumanInTheLoopMiddleware(\n",
    "            interrupt_on={\n",
    "                \"send_mail_tool\":\n",
    "                    {\n",
    "                        \"allowed_decisions\":[\"approve\",\"edit\",\"reject\"],\n",
    "\n",
    "                    },\n",
    "                    \"read_mail_tool\": False\n",
    "            }\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "config ={'configurable':{'thread_id':'test-3'}}\n",
    "result = agent.invoke({\"messages\" : HumanMessage(content=\"send email to test@example.com with subject test and body testsome\")},config)\n",
    "\n",
    "if \"__interrupt__\" in result:\n",
    "    print(\"Paused. Approving...\")\n",
    "    resoo = agent.invoke(Command(\n",
    "        resume ={\n",
    "            \"decisions\":[{\n",
    "                'type':'edit',\n",
    "                'edited_action':{\n",
    "                    'name':'send_mail_tool',\n",
    "                    'args':{\n",
    "                        'recipent':'abct@example.com',\n",
    "                        'subject':'testabc',\n",
    "                        'body':'testsomeabc'\n",
    "                    }\n",
    "                }\n",
    "        }]\n",
    "        }\n",
    "    ),config=config)\n",
    "    print(f\"Result: {resoo['messages'][-1].content}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
